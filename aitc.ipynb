{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bd9d525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"test.csv\", encoding='unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "326671d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_list = df['Questions'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f2f4171",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_list = df['Answers'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccad1618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nikc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nikc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nikc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8196a5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def preprocess(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove non-alphanumeric characters\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in lemmatized_tokens]\n",
    "    return ' '.join(stemmed_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "539fb1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_with_stopwords(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove non-alphanumeric characters\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in lemmatized_tokens]\n",
    "    return ' '.join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e03bee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_text: about aitc\n",
      "similarities: [[0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "max_similarity: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikc\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I can't answer this question.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=nltk.word_tokenize)\n",
    "X = vectorizer.fit_transform([preprocess(q) if isinstance(q, str) else \"\" for q in questions_list])\n",
    "\n",
    "#X = vectorizer.fit_transform([preprocess(q) for q in questions_list])\n",
    "\n",
    "def get_response(text):\n",
    "    processed_text = preprocess_with_stopwords(text)\n",
    "    print(\"processed_text:\", processed_text)\n",
    "    vectorized_text = vectorizer.transform([processed_text])\n",
    "    similarities = cosine_similarity(vectorized_text, X)\n",
    "    print(\"similarities:\", similarities)\n",
    "    max_similarity = np.max(similarities)\n",
    "    print(\"max_similarity:\", max_similarity)\n",
    "    if max_similarity > 0.6:\n",
    "        high_similarity_questions = [q for q, s in zip(questions_list, similarities[0]) if s > 0.6]\n",
    "        print(\"high_similarity_questions:\", high_similarity_questions)\n",
    "\n",
    "        target_answers = []\n",
    "        for q in high_similarity_questions:\n",
    "            q_index = questions_list.index(q)\n",
    "            target_answers.append(answers_list[q_index])\n",
    "        print(target_answers)\n",
    "\n",
    "        Z = vectorizer.fit_transform([preprocess_with_stopwords(q) for q in high_similarity_questions])\n",
    "        processed_text_with_stopwords = preprocess_with_stopwords(text)\n",
    "        print(\"processed_text_with_stopwords:\", processed_text_with_stopwords)\n",
    "        vectorized_text_with_stopwords = vectorizer.transform([processed_text_with_stopwords])\n",
    "        final_similarities = cosine_similarity(vectorized_text_with_stopwords, Z)\n",
    "        closest = np.argmax(final_similarities)\n",
    "        return target_answers[closest]\n",
    "    else:\n",
    "        return \"I can't answer this question.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02153279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_text: data analyt\n",
      "similarities: [[1.         0.32491362 0.39723569 0.3100033  0.3100033  0.50650236\n",
      "  0.37086918 0.37086918]]\n",
      "max_similarity: 1.0000000000000002\n",
      "high_similarity_questions: ['What is data analytics?']\n",
      "['Data analytics is the process of examining raw data to discover meaningful patterns, draw conclusions, and make informed decisions. It involves collecting, transforming, and analyzing large sets of data to extract valuable insights and support decision-making.']\n",
      "processed_text_with_stopwords: data analyt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Data analytics is the process of examining raw data to discover meaningful patterns, draw conclusions, and make informed decisions. It involves collecting, transforming, and analyzing large sets of data to extract valuable insights and support decision-making.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_response('data analytics?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66d14303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ginger in c:\\users\\nikc\\anaconda3\\lib\\site-packages (0.0.4)\n",
      "Requirement already satisfied: csscompressor in c:\\users\\nikc\\anaconda3\\lib\\site-packages (from ginger) (0.9.5)\n",
      "Requirement already satisfied: Jinja2 in c:\\users\\nikc\\anaconda3\\lib\\site-packages (from ginger) (3.1.2)\n",
      "Requirement already satisfied: jsmin in c:\\users\\nikc\\anaconda3\\lib\\site-packages (from ginger) (3.0.1)\n",
      "Requirement already satisfied: libsass in c:\\users\\nikc\\anaconda3\\lib\\site-packages (from ginger) (0.22.0)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\nikc\\anaconda3\\lib\\site-packages (from ginger) (6.0)\n",
      "Requirement already satisfied: watchdog in c:\\users\\nikc\\anaconda3\\lib\\site-packages (from ginger) (2.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nikc\\anaconda3\\lib\\site-packages (from Jinja2->ginger) (2.1.1)\n",
      "Collecting cloudscraper\n",
      "  Using cached cloudscraper-1.2.71-py2.py3-none-any.whl (99 kB)\n",
      "Requirement already satisfied: pyparsing>=2.4.7 in c:\\users\\nikc\\anaconda3\\lib\\site-packages (from cloudscraper) (3.0.9)\n",
      "Requirement already satisfied: requests>=2.9.2 in c:\\users\\nikc\\anaconda3\\lib\\site-packages (from cloudscraper) (2.31.0)\n",
      "Requirement already satisfied: requests-toolbelt>=0.9.1 in c:\\users\\nikc\\anaconda3\\lib\\site-packages (from cloudscraper) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nikc\\anaconda3\\lib\\site-packages (from requests>=2.9.2->cloudscraper) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nikc\\anaconda3\\lib\\site-packages (from requests>=2.9.2->cloudscraper) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nikc\\anaconda3\\lib\\site-packages (from requests>=2.9.2->cloudscraper) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nikc\\anaconda3\\lib\\site-packages (from requests>=2.9.2->cloudscraper) (2023.7.22)\n",
      "Installing collected packages: cloudscraper\n",
      "Successfully installed cloudscraper-1.2.71\n"
     ]
    }
   ],
   "source": [
    "!pip install ginger\n",
    "!pip install cloudscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "55f5ced2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\nikc\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nikc\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nikc\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nikc\\anaconda3\\lib\\site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nikc\\anaconda3\\lib\\site-packages (from requests) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "06d81d33",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 2 (char 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\models.py:971\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    972\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    973\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_decoder\u001b[38;5;241m.\u001b[39mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_decode(s, idx\u001b[38;5;241m=\u001b[39m_w(s, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mend())\n\u001b[0;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 2 (char 1)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat is data scient\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m parse \u001b[38;5;241m=\u001b[39m GingerIt()\n\u001b[1;32m----> 6\u001b[0m corrected_text \u001b[38;5;241m=\u001b[39m parse\u001b[38;5;241m.\u001b[39mparse(text)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(corrected_text[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m~\\AITCChatBot\\gingerit.py:29\u001b[0m, in \u001b[0;36mGingerIt.parse\u001b[1;34m(self, text, verify)\u001b[0m\n\u001b[0;32m     18\u001b[0m session \u001b[38;5;241m=\u001b[39m cloudscraper\u001b[38;5;241m.\u001b[39mcreate_scraper()\n\u001b[0;32m     19\u001b[0m request \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl,\n\u001b[0;32m     21\u001b[0m     params\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m     verify\u001b[38;5;241m=\u001b[39mverify,\n\u001b[0;32m     28\u001b[0m )\n\u001b[1;32m---> 29\u001b[0m data \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(text, data)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\models.py:975\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    972\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    973\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[1;32m--> 975\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmsg, e\u001b[38;5;241m.\u001b[39mdoc, e\u001b[38;5;241m.\u001b[39mpos)\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 2 (char 1)"
     ]
    }
   ],
   "source": [
    "from gingerit import GingerIt\n",
    "\n",
    "text=\"what is data scient\"\n",
    "\n",
    "parse = GingerIt()\n",
    "corrected_text = parse.parse(text)\n",
    "print(corrected_text[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e112b99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80c7d78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
